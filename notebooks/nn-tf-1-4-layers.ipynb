{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9dhL1TlC6t81"},"outputs":[],"source":["IMAGE_SIZE = 224\n","NUM_CHANELLS = 3\n","LEN_FEATURES = IMAGE_SIZE*IMAGE_SIZE*NUM_CHANELLS\n","BATCH_SIZE = 32\n","ZISE_OF_IMAGES = 2000\n","CLASSES = ['happy', 'sad']\n","# DRIVE_PATH = '/content/drive'\n","# PATH = DRIVE_PATH + \"/MyDrive/deep-learning/\"\n","# PATH_TO_DATA = PATH + \"data/images/\"\n","# RES_PATH = PATH+\"RES/\"\n","\n","PATH = r\"D:\\\\Projects\\Development\\\\courses_projects\\\\deep_learning_and_natural_language_processing\\\\dog-image-mood-classification\"\n","PATH_TO_DATA = PATH + '\\\\' + \"data\\\\images\"\n","RES_PATH = r\"results\\\\\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3011,"status":"ok","timestamp":1670930643997,"user":{"displayName":"lang app","userId":"10739013969153975359"},"user_tz":-120},"id":"-_XPr0wm6uy1","outputId":"c059ec88-4ab4-4d11-ca5a-c805912ba9fd"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount(DRIVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3042,"status":"ok","timestamp":1670930647035,"user":{"displayName":"lang app","userId":"10739013969153975359"},"user_tz":-120},"id":"e_zgfYwFGj4-","outputId":"b88430c8-e441-4182-a74d-ba71f599a6f0"},"outputs":[],"source":["# modeling\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","tf.disable_v2_behavior()\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import cv2\n","\n","# tools\n","import os\n","import glob\n","import random\n","import numpy as np\n","from datetime import datetime\n","import pytz\n","import sklearn as sk\n","from tqdm import tqdm\n","\n","# others\n","import warnings\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNncB0hUMoBm"},"outputs":[],"source":["def split_train_test(test_size: float, images, labels, files):\n","  size_of_train = round(len(images) * test_size)\n","  train_images = images[0: size_of_train]\n","  train_labels = labels[0: size_of_train]\n","  train_files = files[0: size_of_train]\n","\n","  test_images = images[size_of_train: -1]\n","  test_labels = labels[size_of_train: -1]\n","  test_files = files[size_of_train: -1]\n","\n","  return [train_images, train_labels, train_files], [test_images, test_labels, test_files]\n","\n","def load_data(classes):\n","  images = []\n","  labels = []\n","  files = []\n","  for i in range(len(classes)):\n","    folder_name = classes[i]\n","    path = os.path.join(PATH_TO_DATA, folder_name)\n","    count = 0\n","    for file_index, file_name in enumerate(os.listdir(path)):\n","      images.append(os.path.join(path, file_name))\n","      # images.append(file_name)\n","      label = [i]\n","      labels.append(label)\n","      files.append(os.path.join(path, file_name))\n","      if count == ZISE_OF_IMAGES:\n","        break\n","      count += 1\n","\n","  return  images, labels, files\n","\n","def generate_batch(file_names, labels, batch_size):\n","  num_of_batch = int(len(file_names)/batch_size)\n","  i = 0\n","  while(True):\n","    if i == num_of_batch:\n","      i = 0\n","    start = i*batch_size\n","    end = (i+1)*batch_size\n","    images = []\n","    for file_name in file_names[start:end]:\n","      image = cv2.imread(file_name)\n","      image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","      images.append(image)\n","    yield np.array(images), np.array(labels[start:end])\n","    i += 1\n","\n","def normalise_data(data):\n","  data = data.astype(np.float32)\n","  for i in range(len(data)):\n","    for j in range(len(data[i])):\n","      data[i][j]/=255.0\n","  return data\n","\n","def plot_images(images, cls_true, cls_pred=None):\n","    \n","    if len(images) == 0:\n","        print(\"no images to show\")\n","        return \n","    else:\n","        random_indices = random.sample(range(len(images)), min(len(images), 9))\n","        \n","    if cls_pred is not None:\n","        images, cls_true, cls_pred  = zip(*[(images[i], cls_true[i], cls_pred[i]) for i in random_indices])\n","    else:\n","        images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n","    \n","    fig, axes = plt.subplots(3, 3)\n","    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n","\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(images[i].reshape(IMAGE_SIZE, IMAGE_SIZE, 3))\n","\n","        if cls_pred is None:\n","            xlabel = \"True: {0}\".format(cls_true[i])\n","        else:\n","            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n","\n","        ax.set_xlabel(xlabel)\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-CtSFFnEuvC"},"outputs":[],"source":["images, labels, files = load_data(CLASSES)\n","images, labels, files = shuffle(images, labels, files)  # shuffle the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-E4NAE2EpSd"},"outputs":[],"source":["train, test = split_train_test(test_size=0.8, images=images, labels=labels, files=files)\n","train_images, train_labels, train_files = train[:]\n","test_images, test_labels, test_files = test[:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670930648600,"user":{"displayName":"lang app","userId":"10739013969153975359"},"user_tz":-120},"id":"A2_FAi2xYiAO","outputId":"4944efaf-436f-4464-e3f9-31a653690c29"},"outputs":[],"source":["# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","# tf.debugging.set_log_device_placement(True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhCXM4YeKaOj"},"outputs":[],"source":["print(f\"Size of train: {len(train_images)}\")\n","print(f\"Size of test: {len(test_images)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_vactors(labels):\n","    new_labels = []\n","    for label in labels:\n","        value = label[0]\n","        new_label = [0] * len(CLASSES)\n","        new_label[value] = 1\n","        new_labels.append(new_label)\n","    return new_labels\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_labels = make_vactors(train_labels)\n","test_labels = make_vactors(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":32294,"status":"error","timestamp":1670930680889,"user":{"displayName":"lang app","userId":"10739013969153975359"},"user_tz":-120},"id":"nbvY3MD4Sq4T","outputId":"3c4289c2-7528-45f1-aab9-860306a818aa"},"outputs":[],"source":["hidden1_size = 2000\n","hidden2_size = 700\n","hidden3_size = 200\n","hidden4_size = 70\n","# [1, 0] [0,1]\n","eps = 1e-12\n","x = tf.placeholder(tf.float32, [None,  LEN_FEATURES], name=\"X\")\n","y_ = tf.placeholder(tf.float32, [None, len(CLASSES)], name=\"y\")\n","\n","W1 = tf.Variable(tf.truncated_normal([LEN_FEATURES, hidden1_size],stddev=0.1))\n","b1 = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n","z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n","\n","W2 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size],stddev=0.1))\n","b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n","z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n","\n","W3 = tf.Variable(tf.truncated_normal([hidden2_size, hidden3_size],stddev=0.1))\n","b3 = tf.Variable(tf.constant(0.1, shape=[hidden3_size]))\n","z3 = tf.nn.relu(tf.matmul(z2,W3)+b3)\n","\n","W4 = tf.Variable(tf.truncated_normal([hidden3_size, hidden4_size],stddev=0.1))\n","b4 = tf.Variable(tf.constant(0.1, shape=[hidden4_size]))\n","z4 = tf.nn.relu(tf.matmul(z3,W4)+b4)\n","\n","W5 = tf.Variable(tf.truncated_normal([hidden4_size, len(CLASSES)],stddev=0.1))\n","b5 = tf.Variable(tf.constant(0.1, shape=[len(CLASSES)]))\n","# pred = 1 / (1.0 + tf.exp(-(tf.matmul(z4,W5) + b5)))\n","pred = tf.nn.softmax(tf.matmul(z4,W5)+b5)\n","# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(pred+eps), reduction_indices=[1]))\n","# loss1 = -(y_ * tf.log(pred + eps) + (1 - y_) * tf.log(1- pred + eps))\n","# loss = tf.reduce_mean(loss1)\n","# update = tf.train.AdamOptimizer(0.0000000001).minimize(loss)\n","# update = tf.train.AdamOptimizer(0.001).minimize(loss)\n","# update = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n","\n","##########################\n","\n","# pred = tf.matmul(z4, W5) + b5\n","\n","cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(pred,y_)\n","cost = tf.reduce_mean(cross_entropy)\n","# update = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n","update = tf.train.AdamOptimizer(0.001).minimize(cost)\n","\n","train_loss_array = []\n","test_loss_array = []\n","epoch_array = []\n","y_1 = tf.matmul(x, W1) + b1\n","y_2 = tf.matmul(y_1, W2) + b2\n","y_3 = tf.matmul(y_2, W3) + b3\n","y_4 = tf.matmul(y_3, W4) + b4\n","y_h = tf.round(tf.sigmoid(y_4))\n","\n","it_batch_train= generate_batch(train_images, train_labels, BATCH_SIZE)\n","it_batch_test = generate_batch(test_images, test_labels, BATCH_SIZE)\n","num_batch_train = int(len(train_images)/BATCH_SIZE)\n","num_batch_test = int(len(test_images)/BATCH_SIZE)\n","timezone = pytz.timezone(\"Israel\")\n","now_time = datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M-%S\")\n","path_to_res = f'{RES_PATH}res_{now_time}.txt'\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","for epoch in tqdm(range(100)):\n","    train_loss, test_loss = 0, 0\n","\n","    # Train\n","    for b in range(num_batch_train):\n","      data_x, data_y = next(it_batch_train)\n","      data_x = data_x.reshape(len(data_x), LEN_FEATURES)\n","      data_x = normalise_data(data_x)\n","      pred_tr, current_train_loss = sess.run([update, cost], feed_dict = {x:data_x, y_:data_y})\n","      train_loss+=current_train_loss\n","\n","    # Test\n","    all_y_test = np.array([])\n","    all_y_pred = np.array([])\n","\n","    for b in range(num_batch_test):\n","      test_x, test_y = next(it_batch_test)\n","      test_x = test_x.reshape(len(test_x), LEN_FEATURES)\n","      test_x = normalise_data(test_x)\n","      pred, current_los = sess.run([y_h, cost], feed_dict = {x: test_x, y_: test_y})\n","      test_loss+=current_los\n","\n","      y_pred = np.array(pred)[:,0]\n","      test_y = np.array(test_y)[:,0]\n","      all_y_pred = np.concatenate((all_y_pred, y_pred))\n","      all_y_test = np.concatenate((all_y_test, test_y))\n","\n","    train_loss /= num_batch_train\n","    test_loss /= num_batch_test\n","\n","    with open(path_to_res, 'a') as f:\n","      print(f\"\\nE: {epoch}   Loss train: {train_loss:.4}   Loss Test: {test_loss:.4}\", file=f)\n","      print(sk.metrics.classification_report(all_y_test.astype(int),all_y_pred.astype(int), target_names=CLASSES), file=f)\n","    train_loss_array.append(train_loss)\n","\n","    if epoch % 10 == 0:\n","      print(f\"\\nE: {epoch}   Loss train: {train_loss:.4}   Loss Test: {test_loss:.4}\")\n","      \n","    test_loss_array.append(test_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwtDN_JFoQEz"},"outputs":[],"source":["plt.title(\"Loss value throw train logistic regression\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(epoch_array, loss_array)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMqRnVnJ2mlo"},"outputs":[],"source":["y_logits = tf.matmul(x, W) + b\n","y_hat = tf.round(tf.sigmoid(y_logits))\n","y_pred = sess.run(y_hat, feed_dict = {x: test_images, y_: test_labels})\n","print(classification_report(test_labels, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4w-TC_F-5iBF"},"outputs":[],"source":["plot_images(test_images, test_labels, y_pred)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"2121b63ebec64736fa1408ad9b4ff63d888044831182c075f83e3f47370387ac"}}},"nbformat":4,"nbformat_minor":0}

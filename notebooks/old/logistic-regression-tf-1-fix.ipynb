{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["IMAGE_SIZE = 224\n","NUM_CHANELLS = 3\n","LEN_FEATURES = IMAGE_SIZE * IMAGE_SIZE * NUM_CHANELLS\n","BATCH_SIZE = 64\n","# ZISE_OF_IMAGES = 4500\n","ZISE_OF_IMAGES = 3000\n","CLASSES = ['happy', 'sad']\n","# DRIVE_PATH = '/content/drive'\n","PATH = r\"D:\\\\Projects\\Development\\\\courses_projects\\\\deep_learning_and_natural_language_processing\\\\dog-image-mood-classification\"\n","PATH_TO_DATA = PATH + '\\\\' + \"data\\\\images\"\n","RES_PATH = r\"results\\\\\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":509,"status":"ok","timestamp":1670960746034,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"e_zgfYwFGj4-"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\kggol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')\n","\n","# tools\n","import os\n","import glob\n","import random\n","import numpy as np\n","from datetime import datetime\n","import pytz\n","import sklearn as sk\n","from tqdm import tqdm\n","\n","# modeling\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","tf.disable_v2_behavior()\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import cv2"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","def split_train_test(test_size: float, images, labels, files):\n","  size_of_train = round(len(images) * test_size)\n","  train_images = images[0: size_of_train]\n","  train_labels = labels[0: size_of_train]\n","  train_files = files[0: size_of_train]\n","\n","  test_images = images[size_of_train: -1]\n","  test_labels = labels[size_of_train: -1]\n","  test_files = files[size_of_train: -1]\n","\n","  return [train_images, train_labels, train_files], [test_images, test_labels, test_files]\n","\n","def load_data(classes):\n","  images = []\n","  labels = []\n","  files = []\n","  for i in range(len(classes)):\n","    folder_name = classes[i]\n","    path = os.path.join(PATH_TO_DATA, folder_name)\n","    count = 0\n","    for file_index, file_name in enumerate(os.listdir(path)):\n","      images.append(file_name)\n","      label = [i]\n","      labels.append(label)\n","      files.append(os.path.join(path, file_name))\n","      if count == ZISE_OF_IMAGES:\n","        break\n","      count += 1\n","\n","  return images, labels, files\n","\n","def generate_batch(file_names, labels, batch_size):\n","  num_of_batch = int(len(file_names)/batch_size)\n","  i = 0\n","  while(True):\n","    if i == num_of_batch:\n","      i = 0\n","    start = i * batch_size\n","    end = (i + 1) * batch_size\n","    images = []\n","    for file_name in file_names[start:end]:\n","      image = cv2.imread(file_name)\n","      image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n","      images.append(image)\n","    yield np.array(images), np.array(labels[start:end])\n","    i += 1\n","\n","def normalise_data(data):\n","  data = data.astype(np.float32)\n","  for i in range(len(data)):\n","    for j in range(len(data[i])):\n","      data[i][j]/=255.0\n","  return data\n","\n","def plot_images(images, cls_true, cls_pred=None):\n","    \n","    if len(images) == 0:\n","        print(\"no images to show\")\n","        return \n","    else:\n","        random_indices = random.sample(range(len(images)), min(len(images), 9))\n","        \n","    if cls_pred is not None:\n","        images, cls_true, cls_pred  = zip(*[(images[i], cls_true[i], cls_pred[i]) for i in random_indices])\n","    else:\n","        images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n","    \n","    fig, axes = plt.subplots(3, 3)\n","    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n","\n","    for i, ax in enumerate(axes.flat):\n","        ax.imshow(images[i].reshape(IMAGE_SIZE, IMAGE_SIZE, 3))\n","\n","        if cls_pred is None:\n","            xlabel = \"True: {0}\".format(cls_true[i])\n","        else:\n","            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n","\n","        ax.set_xlabel(xlabel)\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Prepare data"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":431,"status":"ok","timestamp":1670960752105,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"v-CtSFFnEuvC"},"outputs":[],"source":["images, labels, files = load_data(CLASSES)\n","images, labels, files = shuffle(images, labels, files)  # shuffle the data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670960753772,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"m-E4NAE2EpSd"},"outputs":[],"source":["train, test = split_train_test(test_size=0.8, images=images, labels=labels, files=files)\n","train_images, train_labels, train_files = train[:]\n","test_images, test_labels, test_files = test[:]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670960755278,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"A2_FAi2xYiAO","outputId":"ab0f6354-3365-492a-8e09-ccb49121d914"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of train: 4802\n","Size of test: 1199\n"]}],"source":["print(f\"Size of train: {len(train_images)}\")\n","print(f\"Size of test: {len(test_images)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Utils"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbvY3MD4Sq4T","outputId":"f5a56b28-badf-44b9-fd8d-27031c06f912"},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|          | 1/100 [31:38<52:12:45, 1898.64s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 0 Loss train: 0.841   Loss Test: 0.6741\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 11/100 [5:10:09<41:57:35, 1697.26s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 10 Loss train: 0.6689   Loss Test: 0.6972\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 21/100 [10:11:59<38:59:14, 1776.64s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 20 Loss train: 0.5949   Loss Test: 0.6983\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███       | 31/100 [15:05:42<33:45:13, 1761.06s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 30 Loss train: 0.5445   Loss Test: 0.7633\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 41/100 [20:00:21<28:07:39, 1716.26s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 40 Loss train: 0.5126   Loss Test: 0.816\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████     | 51/100 [23:43:44<17:54:18, 1315.48s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 50 Loss train: 0.4823   Loss Test: 0.8276\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████     | 51/100 [23:48:05<22:52:05, 1680.11s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [7], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m data_x, data_y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(it_batch_train)\n\u001b[0;32m     37\u001b[0m data_x \u001b[39m=\u001b[39m data_x\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(data_x), LEN_FEATURES)\n\u001b[1;32m---> 38\u001b[0m data_x \u001b[39m=\u001b[39m normalise_data(data_x)\n\u001b[0;32m     39\u001b[0m current_train_loss \u001b[39m=\u001b[39m sess\u001b[39m.\u001b[39mrun([update, loss], feed_dict \u001b[39m=\u001b[39m {x:data_x, y_:data_y})[\u001b[39m1\u001b[39m]\n\u001b[0;32m     40\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m current_train_loss\n","Cell \u001b[1;32mIn [3], line 52\u001b[0m, in \u001b[0;36mnormalise_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[0;32m     51\u001b[0m   \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data[i])):\n\u001b[1;32m---> 52\u001b[0m     data[i][j]\u001b[39m/\u001b[39m\u001b[39m=\u001b[39m\u001b[39m255.0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m data\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["eps = 1e-12\n","x = tf.placeholder(tf.float32, [None,  LEN_FEATURES])      \n","y_ = tf.placeholder(tf.float32, [None, 1])\n","W = tf.Variable(tf.zeros([LEN_FEATURES, 1]))\n","b = tf.Variable(tf.zeros([1]))\n","pred = 1 / (1.0 + tf.exp(-(tf.matmul(x,W) + b)))\n","loss1 = -(y_ * tf.log(pred + eps) + (1 - y_) * tf.log(1- pred + eps))\n","loss = tf.reduce_mean(loss1)\n","# update = tf.train.GradientDescentOptimizer(0.000000001).minimize(loss)\n","update = tf.train.AdamOptimizer(0.0001).minimize(loss)\n","\n","train_loss_array = []\n","test_loss_array = []\n","epoch_array = []\n","y_logits = tf.matmul(x, W) + b\n","y_h = tf.round(tf.sigmoid(y_logits))\n","\n","\n","it_batch_train= generate_batch(train_files, train_labels, BATCH_SIZE)\n","it_batch_test = generate_batch(test_files, test_labels, BATCH_SIZE)\n","num_batch_train = round(len(train_images)/BATCH_SIZE)\n","num_batch_test = round(len(test_images)/BATCH_SIZE)\n","timezone = pytz.timezone(\"Israel\")\n","now_time = datetime.now(timezone).strftime(\"%d-%m-%Y_%H-%M-%S\")\n","path_to_res = f'{RES_PATH}res_{now_time}.txt'\n","epoch_array = []\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","for epoch in tqdm(range(100)):\n","    train_loss, test_loss = 0, 0\n","\n","    # Train\n","    for b in range(num_batch_train):\n","      data_x, data_y = next(it_batch_train)\n","      data_x = data_x.reshape(len(data_x), LEN_FEATURES)\n","      data_x = normalise_data(data_x)\n","      current_train_loss = sess.run([update, loss], feed_dict = {x:data_x, y_:data_y})[1]\n","      train_loss += current_train_loss\n","    train_loss /= num_batch_train\n","\n","    # Test\n","    all_y_test = np.array([])\n","    all_y_pred = np.array([])\n","\n","    for b in range(num_batch_test):\n","      test_x, test_y = next(it_batch_test)\n","      test_x = normalise_data(test_x)\n","      test_x = test_x.reshape(len(test_x), LEN_FEATURES)\n","      pred, current_loss = sess.run([y_h, loss], feed_dict = {x: test_x, y_: test_y})\n","      test_loss += current_loss\n","\n","      y_pred = np.array(pred)[:,0]\n","      test_y = np.array(test_y)[:,0]\n","      all_y_pred = np.concatenate((all_y_pred, y_pred))\n","      all_y_test = np.concatenate((all_y_test, test_y))\n","    test_loss /= num_batch_test\n","\n","    with open(path_to_res, 'a') as f:\n","      print(f\"\\nEpoch: {epoch} Loss train: {train_loss:.4}   Loss Test: {test_loss:.4}\", file=f)\n","      print(sk.metrics.classification_report(all_y_test.astype(int),all_y_pred.astype(int), target_names=CLASSES), file=f)\n","\n","    if epoch % 10 == 0:\n","      print(f\"\\nEpoch: {epoch} Loss train: {train_loss:.4}   Loss Test: {test_loss:.4}\")\n","    train_loss_array.append(train_loss)\n","    test_loss_array.append(test_loss)\n","    epoch_array.append(epoch)\n","print(sk.metrics.classification_report(all_y_test.astype(int),all_y_pred.astype(int), target_names=CLASSES))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":34,"status":"aborted","timestamp":1670953324347,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"yh-imnSCUGGt"},"outputs":[],"source":["plt.title(\"Train loss value throw train logistic regression\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(epoch_array, train_loss_array)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":34,"status":"aborted","timestamp":1670953324348,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"kwtDN_JFoQEz"},"outputs":[],"source":["plt.title(\"Test loss value throw train logistic regression\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(epoch_array, test_loss)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":34,"status":"aborted","timestamp":1670953324348,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"KMqRnVnJ2mlo"},"outputs":[],"source":["y_output = tf.matmul(x, W) + b\n","y_sigma = tf.round(tf.sigmoid(y_output))\n","y_pred = sess.run(y_sigma, feed_dict = {x: test_images, y_: test_labels})\n","print(classification_report(test_labels, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":35,"status":"aborted","timestamp":1670953324349,"user":{"displayName":"שלמה גליק","userId":"02945492298563833253"},"user_tz":-120},"id":"4w-TC_F-5iBF"},"outputs":[],"source":["plot_images(test_images, test_labels, y_pred)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"2121b63ebec64736fa1408ad9b4ff63d888044831182c075f83e3f47370387ac"}}},"nbformat":4,"nbformat_minor":0}
